{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc0d9be",
   "metadata": {},
   "source": [
    "# RL w/ Verifiable Rewards Experiments  \n",
    "\n",
    "running locally so bear with me; sticking to smol models and toy-ish tasks to start with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26726907",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a70d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, random, math, json\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa9f36",
   "metadata": {},
   "source": [
    "### measure baseline accuracy (no RL)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7843d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MODEL_ID = os.environ.get(\"MODEL_ID\", \"Qwen/Qwen3-0.6B\")  # swap to ...-Instruct if you have it\n",
    "DEVICE = \"cuda\"\n",
    "DTYPE = torch.float32\n",
    "LOAD_IN_4BIT = True  # flip to False if bitsandbytes is problematic under WSL2\n",
    "N_EVAL = 100  # TODO: increase later\n",
    "\n",
    "# Keep outputs short and verifiable. No chain-of-thought; just the number.\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a calculator. Solve the user's math problem and reply with an integer. \"\n",
    "    \"ASCII characters only, no markdown. \"\n",
    "    \"Your final answer should be on a new line at the end of your response. \"\n",
    ")\n",
    "\n",
    "def build_messages(problem: str):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": problem},\n",
    "    ]\n",
    "\n",
    "# Extract the last integer-like token (handles negatives)\n",
    "NUM_RE = re.compile(r\"[-+]?\\d+\")\n",
    "\n",
    "def parse_answer(text: str) -> int | None:\n",
    "    matches = NUM_RE.findall(text.strip())\n",
    "    if not matches:\n",
    "        return None\n",
    "    return int(matches[-1])\n",
    "\n",
    "def gen_synthetic_math(n: int = 500, seed: int = 0) -> List[Tuple[str, int]]:\n",
    "    rng = random.Random(seed)\n",
    "    items = []\n",
    "    for _ in range(n):\n",
    "        a = rng.randint(-99999, 99999)\n",
    "        b = rng.randint(-99999, 99999)\n",
    "        op = rng.choice([\"+\", \"-\", \"*\"])\n",
    "        if op == \"+\":\n",
    "            ans = a + b\n",
    "            prob = f\"{a} + {b} = ?\"\n",
    "        elif op == \"-\":\n",
    "            ans = a - b\n",
    "            prob = f\"{a} - {b} = ?\"\n",
    "        elif op == \"*\":\n",
    "            # keep magnitudes modest to avoid huge products\n",
    "            a2 = rng.randint(-50, 50)\n",
    "            b2 = rng.randint(-50, 50)\n",
    "            ans = a2 * b2\n",
    "            prob = f\"{a2} * {b2} = ?\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mathematical operation.\")\n",
    "        items.append((prob, int(ans)))\n",
    "    return items\n",
    "\n",
    "def measure_baseline_accuracy():\n",
    "    kwargs = dict(\n",
    "        dtype=DTYPE,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if LOAD_IN_4BIT:\n",
    "        kwargs.update(dict(quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float32)))\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **kwargs)\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token_id = tok.eos_token_id\n",
    "    # Align model/generation config with tokenizer to avoid warnings\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "    model.config.eos_token_id = tok.eos_token_id\n",
    "    model.config.bos_token_id = getattr(tok, 'bos_token_id', None)\n",
    "    if hasattr(model, 'generation_config'):\n",
    "        model.generation_config.pad_token_id = tok.pad_token_id\n",
    "        model.generation_config.eos_token_id = tok.eos_token_id\n",
    "        model.generation_config.bos_token_id = getattr(tok, 'bos_token_id', None)\n",
    "    # Ensure dtype alignment of embeddings + head and retie (eval baseline)\n",
    "    try:\n",
    "        base = model\n",
    "        # (eval baseline) optional dtype diagnostics\n",
    "        base = model\n",
    "        emb_in = base.get_input_embeddings()\n",
    "        emb_out = base.get_output_embeddings() or getattr(base, 'lm_head', None)\n",
    "        try:\n",
    "            print('in_emb:', emb_in.weight.dtype if emb_in is not None else None, 'out_emb:', emb_out.weight.dtype if emb_out is not None else None)\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    model.eval()\n",
    "\n",
    "    data = gen_synthetic_math(n=N_EVAL, seed=123)\n",
    "    correct = 0\n",
    "    samples = []\n",
    "\n",
    "    for i, (problem, ans) in tqdm(enumerate(data)):\n",
    "        messages = build_messages(problem)\n",
    "        text = tok.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n",
    "        )\n",
    "        inputs = tok(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=999,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                eos_token_id=tok.eos_token_id,\n",
    "                pad_token_id=tok.pad_token_id,\n",
    "            )\n",
    "        gen = tok.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        pred = parse_answer(gen)\n",
    "        is_ok = (pred == ans)\n",
    "        correct += int(is_ok)\n",
    "\n",
    "        if i < 10:  # keep a few examples\n",
    "            samples.append({\"problem\": problem, \"gold\": ans, \"raw\": gen.strip(), \"pred\": pred, \"ok\": bool(is_ok)})\n",
    "\n",
    "    acc = correct / len(data)\n",
    "    print(json.dumps({\"model\": MODEL_ID, \"n\": len(data), \"accuracy\": acc, \"samples\": samples}, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "465eb563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8c30d2844246ef8a4025c803773406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"Qwen/Qwen3-0.6B\",\n",
      "  \"n\": 100,\n",
      "  \"accuracy\": 0.83,\n",
      "  \"samples\": [\n",
      "    {\n",
      "      \"problem\": \"-86273 + -29830 = ?\",\n",
      "      \"gold\": -116103,\n",
      "      \"raw\": \"-86273 + -29830 = -86273 - 29830 = -116103.\",\n",
      "      \"pred\": -116103,\n",
      "      \"ok\": true\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"6756 + -30124 = ?\",\n",
      "      \"gold\": -23368,\n",
      "      \"raw\": \"6756 + (-30124) = 6756 - 30124 = -23368.\",\n",
      "      \"pred\": -23368,\n",
      "      \"ok\": true\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"21 * -8 = ?\",\n",
      "      \"gold\": -168,\n",
      "      \"raw\": \"21 * -8 = -168.\",\n",
      "      \"pred\": -168,\n",
      "      \"ok\": true\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"-10661 + -86394 = ?\",\n",
      "      \"gold\": -97055,\n",
      "      \"raw\": \"-10661 + -86394 = -97055\",\n",
      "      \"pred\": -97055,\n",
      "      \"ok\": true\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"-8 * 39 = ?\",\n",
      "      \"gold\": -312,\n",
      "      \"raw\": \"-8 * 39 = -312\",\n",
      "      \"pred\": -312,\n",
      "      \"ok\": true\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"-35731 + -57066 = ?\",\n",
      "      \"gold\": -92797,\n",
      "      \"raw\": \"-35731 + -57066 = -35731 - 57066 = -92797.\",\n",
      "      \"pred\": -92797,\n",
      "      \"ok\": true\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"-2 * -42 = ?\",\n",
      "      \"gold\": 84,\n",
      "      \"raw\": \"-2 * -42 = 84.\",\n",
      "      \"pred\": 84,\n",
      "      \"ok\": true\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"7 * -37 = ?\",\n",
      "      \"gold\": -259,\n",
      "      \"raw\": \"-37 * 7 = -259\",\n",
      "      \"pred\": -259,\n",
      "      \"ok\": true\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"-32 * -34 = ?\",\n",
      "      \"gold\": 1088,\n",
      "      \"raw\": \"-32 * -34 = 1128\",\n",
      "      \"pred\": 1128,\n",
      "      \"ok\": false\n",
      "    },\n",
      "    {\n",
      "      \"problem\": \"-94433 - -23505 = ?\",\n",
      "      \"gold\": -70928,\n",
      "      \"raw\": \"-94433 - (-23505) = -94433 + 23505 = -70928\\n\\nThe final answer is: -70928\",\n",
      "      \"pred\": -70928,\n",
      "      \"ok\": true\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "measure_baseline_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd73d35",
   "metadata": {},
   "source": [
    "### RL w/ Verifiable Rewards (GRPO)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57691fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train GRPO\n",
    "\n",
    "\n",
    "# ----- Config -----\n",
    "MODEL_ID = os.environ.get(\"MODEL_ID\", \"Qwen/Qwen3-0.6B\")  # use base + strict system prompt\n",
    "DTYPE = torch.bfloat16\n",
    "LOAD_IN_4BIT = False          # was being flaky with this enabled\n",
    "SEED = 42\n",
    "\n",
    "# Keep completions tiny; GRPO will sample multiple per prompt (num_generations)\n",
    "MAX_PROMPT_TOK = 128\n",
    "MAX_COMPLETION_TOK = 128\n",
    "NUM_GENERATIONS = 4          # group size G; 4–8 is typical for small models\n",
    "\n",
    "# ----- Prompt rendering with chat template -----\n",
    "def _pairs_to_rows(pairs: List[Tuple[str, int]]) -> List[Dict[str, int]]:\n",
    "    return [{\"problem\": p, \"gold\": int(g)} for p, g in pairs]\n",
    "\n",
    "def render_prompts(rows: List[Dict[str, int]]) -> Dataset:\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token_id = tok.eos_token_id\n",
    "    tok.padding_side = \"left\"\n",
    "\n",
    "    def _render(row):\n",
    "        problem = row.get(\"problem\", row.get(\"0\"))\n",
    "        gold = row.get(\"gold\", row.get(\"1\"))\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": problem},\n",
    "        ]\n",
    "        # Render to a single prompt string that already includes the assistant tag.\n",
    "        prompt = tok.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n",
    "        )\n",
    "        return {\"prompt\": prompt, \"gold\": gold}\n",
    "\n",
    "    ds = Dataset.from_list(rows)\n",
    "    return ds.map(_render, remove_columns=list(ds.column_names))\n",
    "\n",
    "# Verifiable reward: use same logic as checking base SFT'ed model\n",
    "def reward_correct_integer(completions: List[str], gold: List[int], **kwargs) -> List[float]:\n",
    "    rewards = []\n",
    "    for out, gt in zip(completions, gold):\n",
    "        pred = parse_answer(out)\n",
    "        rewards.append(1.0 if pred == gt else 0.0)\n",
    "    return rewards\n",
    "\n",
    "def train_grpo_integer_math():\n",
    "    random.seed(SEED)\n",
    "\n",
    "    train_rows = _pairs_to_rows(gen_synthetic_math(n=4000, seed=SEED))\n",
    "    eval_rows  = _pairs_to_rows(gen_synthetic_math(n=400,  seed=SEED + 1))\n",
    "    train_ds = render_prompts(train_rows)\n",
    "    eval_ds  = render_prompts(eval_rows)\n",
    "\n",
    "    # LoRA for small, fast updates (QLoRA if LOAD_IN_4BIT=True)\n",
    "    lora = LoraConfig(\n",
    "        r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "    )\n",
    "\n",
    "    # GRPO configuration (grouped sampling, KL via DAPO loss)\n",
    "    args = GRPOConfig(\n",
    "        output_dir=\"qwen3-06b-grpo-math\",\n",
    "        seed=SEED,\n",
    "        tf32=True,\n",
    "        bf16=(DTYPE == torch.bfloat16),\n",
    "        per_device_train_batch_size=16,     # effective batch = this * (world size)\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=10,\n",
    "        save_steps=200,\n",
    "        max_prompt_length=MAX_PROMPT_TOK,\n",
    "        max_completion_length=MAX_COMPLETION_TOK,\n",
    "        # steps_per_generation=None,\n",
    "        num_generations=NUM_GENERATIONS,    # group size G; used for group-relative baseline\n",
    "        temperature=1.0,                    # allow exploration\n",
    "        top_p=0.9,\n",
    "        # GRPO-specific knobs:\n",
    "        beta=0.05,                          # KL strength\n",
    "        epsilon=0.2,                        # clipping (GRPO’s PPO-style clip)\n",
    "        scale_rewards=\"group\",              # subtract group mean (relative baseline)\n",
    "        loss_type=\"dapo\",                   # TRL default; KL in loss\n",
    "        # Optional speed/memory extras:\n",
    "        # use_liger_loss=True,              # enable if you pip-install TRL[liger] from source\n",
    "        # use_vllm=True,                    # if you want an external vLLM server for gen\n",
    "        # vllm_mode=\"colocate\",\n",
    "        # Model init (QLoRA):\n",
    "        model_init_kwargs=dict(\n",
    "            dtype=DTYPE,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            **({\n",
    "                \"quantization_config\": BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float32)\n",
    "            } if LOAD_IN_4BIT else {})\n",
    "        ),\n",
    "    )\n",
    "    args.generation_batch_size = None\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token_id = tok.eos_token_id\n",
    "    tok.padding_side = \"left\"\n",
    "\n",
    "    trainer = GRPOTrainer(\n",
    "        model=MODEL_ID,\n",
    "        reward_funcs=reward_correct_integer,     # custom verifiable reward\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        processing_class=tok,                    # tokenizer for policy/ref processing\n",
    "        peft_config=lora\n",
    "    )\n",
    "    # Align model/generation config with tokenizer to avoid warnings\n",
    "    model = trainer.model\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "    model.config.eos_token_id = tok.eos_token_id\n",
    "    model.config.bos_token_id = getattr(tok, 'bos_token_id', None)\n",
    "    if hasattr(model, 'generation_config'):\n",
    "        model.generation_config.pad_token_id = tok.pad_token_id\n",
    "        model.generation_config.eos_token_id = tok.eos_token_id\n",
    "        model.generation_config.bos_token_id = getattr(tok, 'bos_token_id', None)\n",
    "    # After TRL constructs the trainer (and internally re-created args),\n",
    "    # restore a concrete generation_batch_size if it is None so downstream\n",
    "    # dataloaders and samplers can derive batch/replication correctly.\n",
    "    if trainer.args.generation_batch_size is None:\n",
    "        trainer.args.generation_batch_size = (\n",
    "            trainer.args.per_device_train_batch_size * trainer.args.world_size * trainer.args.steps_per_generation\n",
    "        )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()  # PEFT adapter if LoRA; full if no PEFT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a5ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe05be1e47c54cc3b14ceedda2d26313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c601e5e53f46a5ba62ad1e4db7e7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/miniconda3/envs/torch312/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  30/3000 02:31 < 4:28:30, 0.18 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>-0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>-0.002200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_grpo_integer_math()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7785ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch312)",
   "language": "python",
   "name": "torch312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
